# -*- coding: utf-8 -*-
"""Red_Neuronal(Celsius a Fahrenheit).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XcOPnTe6PcukOqeKGibiZnDKWwpoqZTi
"""

import tensorflow as tf
import numpy as np

celsius = np.array([-40, -10, 0, 8, 15, 22, 38], dtype=float)
fahrenheit = np.array([-40, 14, 32, 46, 59, 72, 100], dtype=float)

#permite hacer las redes neuronales de manera simple, sin usar tanto codigo
#La capa Dense, permite las conexiones desde cada neurona hasta las otras neuronas de la otra capa
#Units= neuronas de la capa
#input_shape= autoregistra la capa de entrada con una neurona
#Sequential = modelo sequential
#capa = tf.keras.layers.Dense(units=1, input_shape=[1])
#modelo = tf.keras.Sequential([capa])

oculta1 = tf.keras.layers.Dense(units=3, input_shape=[1])
oculta2 = tf.keras.layers.Dense(units=3)
salida = tf.keras.layers.Dense(units=1)
modelo = tf.keras.Sequential([oculta1, oculta2, salida])

#Adam= permite a la red saber como ajustar los pesos y sesgos de manera eficiente,
# esto para que mejore poco a poco, y no al contrario
#mean_squared_error= considera que una poca cantidad de erorres grandes, es peor que una gran
# cantidad de errores peque√±os
modelo.compile(
    optimizer=tf.keras.optimizers.Adam(0.1),
    loss='mean_squared_error'
)

print("Comenzando entrenamiento...")
historial = modelo.fit(celsius, fahrenheit, epochs=1000, verbose=False)
print("Modelo entrenado!")

# Resultado de la funcion de perdida
# Para saber que tan mal estan los resultados de la red, en cada vuelta que dio
import matplotlib.pyplot as plt
plt.xlabel("# Epoca")
plt.ylabel("# Magnitud de perdida")
plt.plot(historial.history["loss"])

print("Prediccion")
resultado = modelo.predict([100.0])
print("El resultado es "+ str(resultado)+ "fahrenheit!")

print("Variables del modelo")
#print(capa.get_weights())
print(oculta1.get_weights())
print(oculta2.get_weights())
print(salida.get_weights())